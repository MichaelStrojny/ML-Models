import torch
import torch.nn as nn
import torch.utils.data as data
import math
import matplotlib

# model adds noise to an image (ammount of noise added is based on the value in a list of time steps)
# resblocks extract abstract / high level features from this noisy image. Then, they take a tensor with these feature and construct a tensor of the noise added at every pixel by upscaling)
# by being able to predict what noise was added to the image, model is able to start with a image of pure noise, and remove noise iterativley until it can generate an image

torch.manual_seed(9)
dev = #device

class AddNoise(nn.Module):
  def __init__(self, time_steps):
    super().__init__()
    self.beta = torch.linspace(1e-4, 0.2, time_steps).requires_grad_(False)
    self.alpha = torch.cumprod(1- self.beta, dim=0).requires_grad_(False)

  def forward(self, x):
    E = torch.randn_like(x)
    x = x*(self.alpha[t])**0.5 + E*(1-self.alpha[t])**0.5
    # alpha shrinks influence of original data and increases influence of noise
    return 

class PosEmbeddings(nn.Module):
    def __init__(self, time_steps, chanel_dims):
        super().__init__()
        position = torch.arange(time_steps).unsqueeze(1).float()
        div = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))
        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)
        embeddings[:, 0::2] = torch.sin(position * div)
        embeddings[:, 1::2] = torch.cos(position * div)  # added to every odd channel
        self.embeddings = embeddings

    def forward(self, x, t):
        embeds = self.embeddings[t].to(x.device)
        return embeds[:, :, None, None]
        # 1st 2 dims are time_steps and channels
        # embedding added to chanel dimension. Last 2 singleton dimensions broadcast across chanel dimension
        # so each positional embedding component for each channel is added to all  the spatial locations (h x w) of the corresponding channel

class ResBlock(nn.Module):
  def __init__(self, dropout_prob, group_size, chan):
    super().__init__()
    self.dropout_prob = dropout_prob
    self.group_size = group_size
    self.Conv1 = torch.conv2D(chan, chan*3, kernel_size=3, padding = 1)
    self.Conv2 = torch.conv2D(chan * 3, chan, kernel_size=3, padding = 1)
    self.relu = nn.reLU(inplace=True)
    self.gnorm1 = nn.GroupNorm(group_size, chan)
    self.gnorm2 = nn.GroupNorm(group_size, chan * 3)
    self.dropout = nn.Dropout(p=dropout_prob, inplace=True)

  def forward(self, x, embeddings):
    x += embeddings[:, :x.shape[1], :, :]    #slicing ensures embeddings match num of channels in x. since num channels changes
    r = self.conv1(self.relu(self.gnorm1(x)))
    r = self.dropout(r)
    r = self.conv2(self.relu(self.gnorm2(r)))
    return r + x

    # norm data, apply relu for non lonearity (norm prevents relu from causing vanishing gradients)
    # more channels created by conv1. capture more complex features. then condensed to focus on most relavebt features (learns to select most relavent features)
    # Each output channel typically corresponds to a different filter that learns to recognize specific patterns.

class Attention(nn.Module):
    def __init__(self, chan, num_heads, dropout_prob):
        super().__init__()
        self.KQVmatrix = nn.Linear(C, C*3)
        self.NeuralNet = nn.Linear(C, C)
        self.H = num_heads
        self.dropout_prob = dropout_prob

    def forward(self, x):
        b, c, h, w = x.shape[]
        x = torch.reshape(torch.transpose(x, 1, 3), (b, h*w, c))
          #(batch_size, channels, height, width) to (batch_size, height * width, channels)
          # done because attention mechanism requires a SEQUENCE of features
          # this new shape litteraly has batch size times h*w rows (each is a pixel) each which has a colum with that pixel's value for every channel
        x = self.KQVmatrix(x)
          # channel features triple to make key , query and value
        x = torch.reshape(x, (b, h*w, 3, self.H, c//(3*self.H)))
        x = torch.transpose(torch.transpose(torch.transpose(x, 1, 2), 0, 1), 3, 4)
        q,k,v = x[0], x[1], x[2]
        #each has dims (b, self.H, h*w, c//(3*self.H))
        x = nn.Softmax(torch.matmul((q, k.T)/(x.size(dim=4))), dim=1)*v
        # return (b, self.H, h*w, c//(3*self.H))
        x = torch.reshape(torch.transpose(x, 2,3), (b, h, w, c*3*self.H))
        # return (b, h, w, c)
        x = self.NeuralNet(x)
        return torch.transpose(torch.transpose(x, 1, 3), 2, 3)
        # ( b C h w )

class UNETblock(_
  def __init__(upscale, attention, num_groups, dropout_prob, num_heads, chan):
    super().__init__()
     self.ResBlock1 = ResBlock(C=chan, num_groups=num_groups, dropout_prob=dropout_prob)
     self.ResBlock2 = ResBlock(C=chan, num_groups=num_groups, dropout_prob=dropout_prob)
     if upscale:
        self.conv = nn.ConvTranspose2d(chan, chan//2, kernel_size = 3, stride = 2, padding = 1)
        # large kernel allows transposed conv to create smoother transition and capture more spatial context
        # kernel of size 3 with stride 2 doubles the spatial dimensions of the input
        # transpose conv = reverse of conv
     else:
        





    
