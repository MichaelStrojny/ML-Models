import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import math
import tqdm
import matplotlib.pyplot as plt
import os

class TransformerTrainingData(data.Dataset):
  def __init__(self, src, tgt):
    self.src = src
    self.tgt = tgt

  def __len__(self):
    return len(self.src)

  def __getitem__(self, idx):
    return self.src[idx], self.tgt[idx]

class MultiHeadAttention(nn.Module):
    def __init__(self, input_dims, num_heads):
      super(MultiHeadAttention, self).__init__()
      self.input_dims = input_dims
      self.num_heads = num_heads
      self.key_matrix = nn.Linear(input_dims, input_dims)
      self.query_matrix = nn.Linear(input_dims, input_dims)
      self.value_matrix = nn.Linear(input_dims, input_dims)
      self.out = nn.Linear(input_dims, input_dims)

    def split_heads(self, x):
      batch_size, seq_length, input_dims = x.size()
      return x.view(batch_size, seq_length, self.num_heads, self.input_dims).transpose(1, 2)
      # view gets us a tensor of n batches of n sequences where each encoding is split into n heads and has n dimensions
      # transpose gets us n batches of n heads where each head contains a split sequence

    def combine_heads(self, x):
      batch_size, num_heads, seq_length, input_dims = x.size()
      return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.input_dims)
      # after a transpose, the tensor's data is no longer contiguous

    def scaled_dot_product_attention(self, Q, K, V, src_mask=None, tgt_mask=None):
      attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  #batch, heads, seqlen, seqlen (last two dims = query, key)
      if src_mask is not None:
        attn_scores = attn_scores.masked_fill(src_mask == 0, -1e9) #replace elements where src mask == 0 with -1e9
      if tgt_mask is not None:
        attn_scores = attn_scores.masked_fill(src_mask == 0, -1e9)
      attn_probs = torch.softmax(attn_scores, dim=-1)    # softmax applied to the key dimension for each query token. ensuring that for each query token, the probabilities assigned to all key tokens sum to 1 (not the same other way around). dim = n. every point (x, y, z..) across channel (dimension n) sums up to 1. only across this direction
      # This normalization process ensures that the attention weights reflect a valid probability distribution for how much focus each query token should give to each key token
      output = torch.matmul(attn_probs, V)
      return output

    def forward(self, Q, K, V, src_mask=None, tgt_mask=None):
      Q = self.split_heads(self.query_matrix(Q))
      K = self.split_heads(self.key_matrix(K))
      V = self.split_heads(self.value_matrix(V))
      output = self.out_proj(self.combine_heads(self.scaled_dot_product_attention(Q, K, V, src_mask, tgt_mask)))
      return output

      #take in sequence and output a sequence with better representation of each token
      # each output embedding token will only contain context from (1) non padding tokens and (2) tokens that are behind it if there is a tgt_mask
      # as such, predictions generated from each token will only be based on that context


class PositionWiseFFN(nn.Module):   # each token put into this FNN individually and outputed individually
  def __init__(self, input_dims, ff_dims):
    super(PositionWiseFFN, self).__init__()
    self.fc1 = nn.Linear(input_dims, ff_dims)
    self.fc2 = nn.Linear(ff_dims, input_dims)
    self.relu = nn.ReLU()

  def forward(self, x):
    return self.fc2(self.relu(self.fc1(x)))

class PosEncode(nn.Module):
  def __init__(self, seq_length, input_dims):
    super(PosEncode, self).__init__()

    pe = torch.zeros(seq_length, input_dims):
    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, input_dims, 2).float() * -(math.log(10000.0) / input_dims))
    # div term creates a tensor of every 2nd dimension in the embedding. frequencies decrease as dimensions increase. different dimensions have different frequencies of oscillation for the sine and cosine functions
    # final vector has half as many elements as there are dimensions. because it adjusts either only the odd or even dimensions
    pe[:, 0::2] = torch.sin(position * div_term) #1st dim is batch
    pe[:, 1::2] = torch.cos(position * div_term)
    # : selects all rows. 0::2 is slicing notation. Start at 0 and choose every 2nd element
    # adds a positional encoding vector every 2 encodings in each embedding in each sequence element
    # applies torch.sin(position * div_term) to every even term in every embedding in the sequence
    # These functions encode positional information in a way that is largely orthogonal to the information conveyed by the actual embedding. This means that the positional encoding provides additional, non-overlapping information rather than interfering with the original embedding data.

    self.register_buffer('pe', pe.unsqueeze(0)) #part of class's state but not a trainable parameter. Basically a parameter but no grad tracking

  def forward(self, x):
    return x + self.pe[:, :x.size(1)]
    #adding positional encoding to the input embeddings does not increase the length of the embeddings

class EncoderLayer(nn.Module):
  def __init__(self, input_dims, num_heads, ff_dims):
    super(EncoderLayer, self).__init__()
    #initialize all the components
    self.attention = MultiHeadAttention(input_dims, num)
    self.ffn = PositionWiseFFN(input_dims, ff_dims)
    self.pos_encode = PosEncode(input_dims)
    self.norm1 = nn.LayerNorm(input_dims)
    self.norm2 = nn.LayerNorm(input_dims)  #layer normalize

   #Layer normalization normalizes the data across all features for each individual example. Batch normalization normalizes the data across the batch for each feature.

  def Forward(self, x, mask=None):
    attn_output = self.attention(x, x, x, mask)
    x = self.norm1(x + self.dropout(attn_output)) # skip layer + norm
    ffn_output = self.ffn(x)
    x = self.norm2(x + self.dropout(ffn_output))
    return x

# self.dropout()
# if you put it in forward class (acting on some elements) it
# will randomly set some of those elements to zero in the forward pass during training
# if self.norm1(x + self.dropout(attn_output))  outputs a tensor, some tensor elements wil be 0

class DecoderLayer(nn.Module):
  def __init__(self, input_dims, num_heads, ff_dims, dropout_rate):
    super(DecoderLayer, self).__init__()
    self.self_attention = MultiHeadAttention(input_dims, num_heads)
    self.cross_attention = MultiHeadAttention(input_dims, num_heads)
    self.ffn = PositionWiseFFN(input_dims, input_dums)
    self.norm1 = nn.LayerNorm(input_dims)
    self.norm2 = nn.LayerNorm(input_dims)
    self.norm3 = nn.LayerNorm(input_dims)
    self.dropout = nn.Dropout(dropout_rate)
    #initialized every class
    def forward(self, x, enc_output, src_mask, tgt_mask):
      attn_output = self.self_attention(x, x, x, tgt_mask=tgt_mask)
      x = self.norm1(x + self.dropout(attn_output))
      attn_output = self.cross_attention(enc_output, enc_output, x, src_mask, tgt_mask) #ensure padding tokens in src and tgt & future tokens in tgt arent attended to
      x = self.norm2(x + self.dropout(attn_output))
      ffn_output = self.ffn(x)
      x = self.norm3(x + self.dropout(ffn_output))
      return

      #purpose of src mask. : If the source sequences have different lengths, padding is often added to make them uniform in length. During the cross-attention phase, we don't want the model to attend to these padding tokens, as they don't carry meaningful information. scr mask turns these tokens into value that is ignored
      # Cross-attention allows the decoder to focus on specific parts of the encoder's output while generating the target sequence. It helps the decoder use information from the source sequence to make more informed predictions.

class Transformer(nn.Module):
  def __init__(self, src_vocab_size, tgt_vocab_size, input_dims, num_heads, num_layers, max_seq_length, dropout):
    super(Transformer, self).__init__()
    encoder_embeddings = nn.Embedding(src_vocab_size, input_dims)
    decoder_embeddings = nn.Embedding(tgt_vocab_size, input_dims)
    # For nn.Embedding, calling the instance with token indices retrieves the corresponding rows from the embedding matrix.
    # encoder embedding  = essentially a matrix where each row represents the embedding of a specific token from the source vocabulary
    # it is optimized to capture the semantics of the input tokens and their relationships within the source sequence
    # decoder embeddings are optimized to generate tokens based on the context provided by the encoder and the previously generated tokens
    # the decoder_embeddings layer is used to convert token IDs from the target vocabulary into dense vectors that the decoder can process
    #When you create an nn.Embedding layer, PyTorch initializes the weights of the embedding matrix with random values. These values are then updated during the training process to learn better representations of the tokens based on the task at hand.
    self.encoder = nn.Modulelist([EncoderLayer(input_dims, num_heads, ff_dims) for _ in range(num_layers)])
    #creates a num_layers quantity of encoderlayers and stores them in a list specially designed for modules (module list
    self.decoder = nn.Modulelist([DecoderLayer(input_dims, num_heads, ff_dims, dropout) for _ in range(num_layers)])

    self.pos_encoder = PosEncode(max_seq_length, input_dims)
    self.pos_decoder = PosEncode(max_seq_length, input_dims)

    self.fc = nn.Linear(input_dims, tgt_vocab_size)
    self.dropout = nn.Dropout(dropout)

    def generate_mask(src, tgt):
      # mask type #1 --> padding masks
      src_mask = (src != 0).unsqueeze(1).unsqueeze(2) #boolean mask. Expected size: [batch_size, 1, 1, sequence_length]
      tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) # [batch_size, 1, tgt_length, 1]
      # mask type #2 --> ensure model does not attend to future tokens
      seq_length = tgt.size(1)
      nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool
      # type as = make it same data type. add 1 dimension so it can be broadcasted over batches.
      # The broadcasting rules allow tensors with fewer dimensions to be expanded to match the shape of tensors with more dimensions.
      # The broadcasting rule here will expand the mask to match the [batch_size, num_heads, seq_length, seq_length] shape
      # same effect as (1, 1, seq_length, seq_length)
      tgt_mask = tgt_mask & nopeak_mask   # & opperation on both tensors. If both corresponding elements are True, then its True. Else, False
      # in other words, must be (1) not padding and (2) not future token to be attended to
      return src_mask, tgt_mask

    def forward(self, src, tgt):        #to begin generating, src = "[Start]".  start token
      # src = input sequence. tgt = sequence to be generated
      src_mask, tgt_mask = generate_mask(src, tgt)
      src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))
      tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))
      # Dropout helps the embeddings learn more robust and generalized features, rather than memorizing specific training examples.
      # Without dropout, embeddings can become too specialized or memorized based on the training data. overfitting

      #decoder is given a sequence.
      # For each position in the target sequence, the decoder generates a probability distribution over the vocabulary for the next word only using the words before it (due to mask)

      for i in self.encoder:
        src = i(src, src_mask)
      for i in self.decoder:
        tgt = i(tgt_embedded, src, src_mask, tgt_mask)   #(batch_size, seq_length, input_dims) -> (batch_size, seq_length, vocab_size)
      return tgt

def Loss_fn(model, tgt, preds):
  ce_loss = torch.nn.CrossEntropyLoss(ignore_index=0)
  return ce_loss(preds, tgt[:, 1:])

# model
model = Transformer(50000, 50000, 561, 12, 6, 500, 0.01)

# data processing
data = TransformerTrainingData(src, tgt)    #fill with src and tgt tensors from your own training data
train_data, test_data = torch.train_test_split(data, test_size=0.2)
test_data = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)
# Each batch is a tuple where the first element is a tensor of source sequences and the second element is a tensor of target sequences.
train_data = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=False)

# training
epochs = 100
model.train()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epoch_losses = []
for epoch in tqdm(range(epochs)):
  epoch_loss = 0
  for src, tgt in train_data:
    preds = model(src, tgt[:, :-1])
    loss = Loss_fn(model, tgt, preds)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    epoch_loss += loss.item()
  epoch_loss = epoch_loss / len(dataloader)
  epoch_losses.append(epoch_loss)

#testing
model.eval()
with torch.inference_mode():
  test_loss = 0
  test_accuracy = 0
  for src, tgt in test_data:
      preds = model(src, tgt[:, :-1])
      loss = Loss_fn(model, tgt, preds)
      test_loss += loss.item()
      accuracy += torch.sum((preds - tgt[:, 1:]).abs()).item() // torch.numel(preds).item()        # (batch_size, seq_len, input_dims)
test_loss = test_loss / len(test_data)
test_accuracy = test_accuracy / len(test_data)

#plot summary of train / test
plt.figure()
plt.plot(epoch_losses, label = 'Training Loss', color = 'blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train & Test Summary')
plt.legend()
plt.show()
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Epochs: {epochs}')

#save model
filename = ""
directory = ""
if not os.path.exists(directory):
  os.makedirs(directory)
PATH = os.path.join(directory, filename)
torch.save(model.state_dict(), PATH)
print(f"Model Dictionary Saved In: {PATH}")
